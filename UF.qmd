---
title: "Automating antibiotic treatment choice using clinical prediction modelling: a microsimulation study"
format:
  docx:
    reference-doc: doc_template.docx
bibliography: UF_references.bib
csl: /Users/alexhoward/Zotero/styles/the-lancet-infectious-diseases.csl
editor: visual
---

Alex Howard\*\
\*corresponding author alexander.howard\@liverpool.ac.uk\
\*\*joint senior author

```{r, echo=FALSE,message=FALSE,warning=FALSE}

library(tidyverse)
library(knitr)
library(glue)

```

# Summary

## Background

Antibiotic prescribing is a key driver of antimicrobial resistance. Machine learning can help predict antimicrobial resistance – to enable AI-driven personalised antibiotic decision making that is implementable in real-world settings, these predictions need to be tied to a measure of drug value (or utility) that reflects real-world clinical priorities and evolving antimicrobial resistance patterns. Here, we used the results of a structured antibiotic ranking tool undertaken by real-world prescribers to develop and test utility functions that facilitate the development of artificial intelligence algorithms for directing antibiotic treatment and antimicrobial susceptibility testing – the aim of this approach was to maximise the projected benefits and practicality of antibiotic treatment for urinary tract infection while minimising harm and generation of antimicrobial resistance.

## Methods

Logistic regression models were trained, tested and validated to predict the risk of antimicrobial resistance, sepsis adverse outcomes, Clostridioides difficile diarrhoea, and drug toxicity for a range of antimicrobial agents in patients with culture-positive urine specimens using open-source real-world pseudonymised electronic healthcare records. Prescribers from a range of primary and secondary care settings undertook a ranking exercise with fictional antimicrobial agents – a ranked logit method was used to determine the relative importance of different antimicrobial characteristics (e.g., oral bioavailability, toxicity, antimicrobial resistance risk) to prescribers. Two utility functions were written that combined these importance weights with probabilities from the logistic regression models to produce an antimicrobial utility value for each patient – one for antimicrobial susceptibility testing, another for antimicrobial treatment. These utility values were then used to provide recommendations for urinary tract infection antimicrobial treatment and susceptibility testing in a microsimulation study design – the antimicrobial choices, resistance patterns, available routes of administration, and World Health Organisation Access, Watch, Reserve category of these recommendations were compared against a standard formulary treatment approach. Sensitivity analyses were performed to examine the performance of the utility function across a range of parameters and clinical settings.

## Findings

2,646 patients who had a urine specimen sent between 2009 and 2018 were assessed in the microsimulation study. The treatment utility function recommended nitrofurantoin as first-line treatment in 92.6% (n=2,451) of cases, a WHO Access agent in 99.8% (n=2,642) of cases, an orally-bioavailable agent in 98.2% (n=2,599) of cases, and an intravenously-administrable agent in 7.3% (n=194) of cases, resulting in coverage of 72.4% (n=1,915) of urine isolates that were subsequently grown. Increasing the importance weight of predicted resistance caused the utility function to recommend piperacillin-tazobactam as first-line treatment in 59.6% (n=1,733) of cases, a WHO Access category agent in 27.7% (n=733) of cases, an orally-bioavailable agent in 30.7% (n=813) of cases, and an intravenously-administrable agent in 93.4% (n=2,471) of cases, resulting in coverage of 87.5% (n=2,316) of urine isolates subsequently grown. Simulating an increase in nitrofurantoin resistance rates to 99.3% caused the utility function to recommend trimethoprim-sulfamethoxazole as first-line treatment in 48.6% (n=1,286) of cases, a WHO Access category agent in 99.9% (n=2,643) of cases, an orally-bioavailable agent in 96.1% (n=2,544) of cases, and an intravenously-administrable agent in 100.0% (n=2,646) of cases, resulting in coverage of 64.0% (n=1,693) of urine isolates subsequently grown. On average, the antimicrobial susceptibility testing utility function provided one more susceptible result per 6-agent panel for WHO Access category agents (median 4 \[IQR 3-5\] versus median 3 \[IQR 1-3\]) than a standard panel approach, provided at least one susceptible result for an Access agent in 95.9% (n=2538) of cases, at least one susceptible result of any kind in 97.1% (n=2,569) of cases, and a result of any kind for the antimicrobial agent the patient was currently prescribed in 72.8% (n=485) of prescriptions.

## Interpretation

Our prescribed-derived antimicrobial utility functions could enable true AI-driven decision making in antimicrobial treatment and susceptibility testing and help nations meet the United Nations target of 70% WHO Access agent prescribing without compromising on treatment efficacy. The prescriber-derived approach is flexible enough to be adaptable to a range of healthcare settings by allowing local clinician factors such as cost and patient cohort to influence the weights of different factors, as well as local epidemiology of resistance, toxicity, C difficile, and sepsis outcomes. Strengthening the prediction model by incorporating more healthcare data from local settings is likely to further improve performance.

## Funding

This research was funded in part by the Wellcome Trust grant ref: 226691/Z/22/Z. For the purpose of open access, the author has applied a CC BY public copyright licence to any Author Accepted Manuscript version arising from this submission. Office for Life Sciences Data-Action Accelerator award also supported this work. The funders had no role in the conceptualisation, design, data collection, analysis, decision to publish or preparation of the manuscript.

# Introduction

## Background

Functioning antibiotics underpin functioning healthcare systems - antimicrobial resistance (AMR) is therefore a significant threat to the delivery of global healthcare. In September 2024, the United Nations General Assembly (UNGA) committed to ensuring that the safest, cheapest antibiotics with the lowest AMR risk (World Health Organisation \[WHO\] Access Category antibiotics) make up 70% of global antibiotic use in healthcare by 2030. The challenge in achieving this goal is determining when it is safe to empirically use Access agents, which often have relatively narrow spectra of activity and will therefore be avoided in favour of more harmful Watch category agents to ensure treatment efficacy where there is doubt about the presence of resistance and/or perceived high stakes (e.g., systemically unwell patients).

Automating antimicrobial treatment choice using machine learning is therefore attractive because algorithms can better quantify the probability of resistance and treatment outcomes using clinical prediction models built on drug, patient, and pathogen characteristics. Algorithms, however, cannot judge the subjective value (or cost) of treatment outcomes to patients, populations, and healthcare systems. To make good antibiotic treatment decisions, algorithms need a mechanism to quantify the value of antibiotic treatment mathematically - a so-called utility function. An automated system that makes antibiotic treatment choices using such a utility function could give prescribers the confidence to use Access category antibiotics when safe to do so, while compromising on Access antibiotic use in situations where treatment efficacy needs to be prioritised (e.g., where risk of resistance and/or clinical deterioration from untreated infection is high).

Here, we describe a utility function that combines clinical prediction models with prescriber-informed outcome values/costs to automate the process of antibiotic treatment choice for urinary tract infection (UTI) to help meet the UNGA 70% global Access antibiotic target without compromising individual patient safety.

# Methods

## Data Sources and Participants

The study complied with the MIMIC-IV dataset Data Use Agreement 1.5.0 and Health Data License. Approval was also obtained for the discrete choice experiment from the NHS Health Research Authority (HRA). A proportionate review was undertaken by a UK Research Ethics Committee (REC), which determined that full REC approval was not required. Clinical prediction models were developed using the open-source PhysioNet dataset MIMIC (Medical Information Mart for Intensive Care)-IV version 2.2, a pseudonymised inpatient and outpatient electronic healthcare record dataset for patients over the age of 18 who were admitted to intensive care or the emergency department between 2008 and 2019 at Beth Israel Deaconess Medical Center (BIDMC - Boston, MA) (<https://physionet.org/content/mimiciv/2.2/>).

Prediction models of antibiotic resistance were developed on urine specimen results with bacterial growth from the `microbiologyevents` dataset (inpatients and outpatients), and prediction models of antibiotic outcomes were made using prescription data from the `prescriptions` dataset (inpatients only). Antimicrobial ssuceptibility interpretations in the microbiologyevents dataset are likely to be based on Clinical Laboratory Standards Institute clinical breakpoints given the U.S. setting \[ref\]. Sample size was determined by the size of the available dataset, and provided a sufficient number of cases per independent variable in the model to reduce the probability of overfitting \[ref\]. Antibiotic financial cost was determined using the lowest procurement/tender award price for each agent in U.S. dollars as listed on the U.S. Department of Veterans Affairs National Acquisition Center (<https://www.vendorportal.ecms.va.gov/NAC/>) when accessed in November 2024.

Antibiotic treatment value was determined using a discrete choice experiment survey completed online between June 1st 2024 and October 31st 2024 by antibiotic prescribers from general practice (GP), pharmacy, medicine, surgery, intensive care, and infectious diseases/medical microbiology working for healthcare providers serviced by diagnostic laboratories at Liverpool University Hospitals NHS Foundation Trust (Liverpool Clinical Laboratories). Sample size was determined by the number of respondents in the 4-month survey period, and provided a sufficient number of cases per independent variable in the model to reduce the probability of overfitting \[ref\].

## Data preparation

Data preprocessing and quality checking was performed in a similar way across sociodemographic groups using RStudio version 2023.12.1+402, and followed a similar pattern to our previous work with the dataset with some additions \[ref\]. The preprocessing workflow is summarised in Appendix 1. Data were quality checked at each stage throughout preprocessing using counts of organism species, results, antimicrobial names, missing data, numbers of rows and numbers of columns to detect preprocessing errors. Choices of predictor variables and their time horizons were based on indirect or direct causal plausibility and/or association with outcome variables \[ref\]. Allocation to training and validation datasets was performed individually for each model by random 80:20 split without replacement, stratified to maintain similar proportions of the outcome in the training and validation datasets.

39 outcomes were required for 39 individual models - 13 individual antibiotic susceptibility prediction models and 24 antibiotic combination resistance models on the urines dataset, and a CDI prediction model and antibiotic toxicity prediction model on the prescriptions dataset. The outcome for all antibiotic susceptibility prediction models was an 'S' result indicating susceptibility of the organism grown in that urine specimen to that antibiotic or antibiotic combination. The outcome for a *Clostridioides difficile* infection (CDI) prediction model was a positive *C. difficile* stool result within 3 months of the start date/time of an antibiotic or antibiotic combination. The outcome for an antibiotic toxicity prediction model was a composite outcome of either stage 3 acute kidney injury (a new increase in serum creatinine to at least three times baseline or at least 3.54µmol/dL in the absence of co-administration of nephrotoxic drugs as defined by the British National Formulary \[ref\] or intravenous contrast during the associated hospital admission), deranged liver function tests (a result newly above the upper end of the normal range for alanine aminotransferase, aspartate aminotransferase, or alkaline phosphatase in the absence of previous coded chronic liver disease or biliary instrumentation in the associated hospital admission), marrow suppression (new anaemia, leukopenia, or thrombocytopenia in the absence of co-administration of cytotoxic drugs as defined by the British National Formulary \[ref\]) or coded bleeding diagnosis in the associated hospital admission) in the seven days following the start date of an antibiotic or a coded antibiotic adverse event for the associated hospital admission. The selection process for predictor and outcome variables was undertaken by the lead author (AH \[Consultant in Medical Microbiology, male, 30s, white British\]) and reviewed by medically-qualified co-authors (CB \[Consultant in Medical Microbiology, female, 30s, white British\], AG \[Consultant in Medical Microbiology, male, 30s, Maltese\], WH \[Consultant in Medical Microbiology, male, 50s, white Australian\], IB \[Consultant in Public Health, male, 50s, white British\]). Outcome and predictor variables were selected consistently across sociodemographic groups. No blinding to allocation or predictor/outcome assessment was implemented at any stage.

## Clinical prediction models

We have described clinical prediction models of resistance for 12 of the 13 antimicrobial agents using logistic regression in our previous work \[ref\]. For this study, XGBoost (an ensemble method that improves predictive accuracy by sequentially combining multiple decision trees) was used instead because it was found to achieve higher initial area under the receiver operating characteristic curve (AUROC) values for the majority of the 12 clinical prediction models when trained and validated on the same datasets. The same XGBoost technique was used for all 39 models. The output of each model was the probability of the binary outcome's positive class (an 'S' result, CDI, or antibiotic toxicity).

XGBoost models were trained and tuned on the training datasets using L2 regularisation (ridge penalty) to control overfitting. AUROC was used as the model evaluation metric. Hyperparameter tuning was performed sequentially for each of the 39 models across parameters in 3 stages. 10 sets of paired values for maximum tree depth (selecting values between two and nine inclusive) and minimum child weight (selecting values between one and 10 inclusive) were randomly selected using latin hypercube sampling with `randomLHS()` from the `lhs` package \[ref\] - a 5-fold cross-validation was then performed across 50 boosting rounds for each of these 10 pairs of values, with a default learning rate of 0.05, a proportion of cases used for each decision tree (subsample rows ratio) of 0.8, and a proportion of predictors used for each decision tree (subsample columns ratio) of 0.8. The maximum tree depth and minimum child weight values that produced the highest AUROC were carried forward to another set of 5-fold cross-validations with 50 boosting rounds across 10 paired subsample rows and columns ratios (again randomly selected using `randomLHS`, with both values sampling from a range between 0.5 and 1 inclusive) with a learning rate of 0.05. The subsample rows and columns ratios that produced the highest AUROC were carried forward to a final set of 5-fold cross-validations across four learning rates (0.1, 0.05, 0.01, and 0.001), with a maximum number of boosting rounds of 1000 and early stopping if AUROC did not improve for 50 rounds. The learning rate and the round number at which the best AUROC value occurred (forming the new total number of rounds parameter) were then taken forward as parameters for training the final model. Class imbalance methods (e.g., class weighting) were not used to avoid inflating the probability of rare occurrences (e.g., CDI) that would subsequently be used to inform the utility function.

Predictor variable contributions to predictive value in model training were measured using Shapley values with `predict(predcontrib=TRUE)` from the `stats` package \[ref\]. Predictor variables with Shapley values of zero were then excluded from models for the final training round. A single validation run was then performed with the final model on the validation dataset, measuring AUROC, accuracy, precision, recall, and F1 score (decision threshold 0.5) using coded formulae for each model. A model stability analysis was performed for each model excluding the 24 antibiotic combination models (15 models in total) in view of the computational time required, where training and validation of the final model was performed in six random train-test splits without replacement (stratified by outcome) for each four smaller train-test dataset size ratios (2:98%, 6:94%, 8:92%, and 12:88%) - AUC, accuracy, precision, recall, and F1 score were measured for each of these validations, and metric distributions were plotted using dot plots to assess heterogeneity in performance. A model fairness analysis was performed where each of the same 15 trained models was validated separately across a range of protected characteristics (race, age, marital status, first language, and gender), with six random-train-test splits per characteristic - AUC, accuracy, precision, recall, and F1 score (decision threshold 0.5) were again measured for each of these validations, and metric distributions plotted using dot plots. Threshold recalibration for fairness was not performed because the output of the models were probabilities rather than classifications. A time cluster analysis was performed to assess out-of-sample performance, where the 15 models were trained on one of four time periods (2008-2010, 2011-2013, 2014-16, and 2017-2019), then validated on that period and the other four time periods, measuring AUC, accuracy, precision, recall, and F1 score (decision threshold 0.5) - this was repeated with six random train-test splits for each pair of time periods, and metrics distributions were plotted using dot plots.

## Prescriber discrete choice experiment

A questionnaire was devised (see Appendix 1) where 13 fictional antibiotics were created and given six characteristics: the AWaRe classification of the antibiotic (yes or no); the CDI risk of the antibiotic (high or low); the toxicity risk of the antibiotic (high or low); whether the antibiotic was only indicated for UTI (Yes or no); whether the antibiotic was orally-administrable (yes or no); whether the antibiotic was intravenously-administrable (yes or no); the financial cost of the antibiotic (high or low). Participants were given a fictional UTI clinical scenario, then asked to rank the treatments in order of suitability. The questionnaire was distributed to participants using SurveyMonkey (<https://uk.surveymonkey.com>) over a 4-month period (1st June 2024 to 21st October 2024) via single organisational and departmental points of contact in infection specialties, medicine, surgery, intensive care, and general practice by email invitation, and written consent for participation was obtained. No minimum sample size was stipulated as over-fitting was not a concern (the approach is designed for the questionnaire to be distributed in the same population in which the algorithm would be used), but the aim was to obtain at least some participants from each specialty.

The ranking of each antibiotic (1-13) by each participant was recorded and converted to a long format where each participant's chosen rank order of antibiotics was treated as a series of choices using `mlogit.data(ranked=TRUE)` from the `mlogit` package \[ref\]. A conditional logit model was built to estimate the probabilities of each observed rank from the choice data based on the six characteristics of each antibiotic (again using `mlogit`). The model was trained to estimate maximum log likelihood using the BFGS method \[ref\]. The coefficients for each characteristic were then extracted from the model and used to represent the relative weight of each antimicrobial characteristic to the expert participants in making an antimicrobial treatment choice for UTI. Coefficients were analysed using data visualisation by bar plot, with 95% confidence intervals approximated using a bootstrap approach with 1000 iterations. A sensitivity analysis was performed where the approach was repeated on subsets of the data to build separate models for infection specialties, medicine, surgery, intensive care, and general practice to compare the weights of the six antimicrobial characteristics to different specialties.

## Utility calculation

Treatment utility was calculated individually for each patient/urine specimen in the microsimulation dataset using the following process:

1.  The trained clinical prediction models were used to predict the probability of antimicrobial susceptibility, producing values between zero and one for ($v_s$), CDI ($v_c$), and toxicity ($v_t$) for each antibiotic (and antibiotic combination) on each urine specimen. These values were then weighted by their respective AUROC values on the test dataset ($a_s$, $a_c$, and $a_t$) to reflect confidence in the probability estimates.

2.  Probabilities were added for each known binary characteristic of antimicrobials - UTI-specificity ($v_u$), oral administration option ($v_o$), and intravenous administration option ($v_i$), with a probability of one for yes and zero for no. Separate binary variables were created for Access ($v_a$) and Reserve ($v_r$) categories, also with a probability of one for yes and zero for no (watch category agents were zero for both).

3.  Financial cost was calculated for each antibiotic using the lowest available cost of an antibiotic in U.S. dollars as listed on the U.S. Department of Veterans Affairs National Acquisition Center - these costs were divided by the highest cost including combinations of antibiotics, producing normalised costs between 0 and 1 ($v_h$).

4.  A personalised utility value was calculated for each antibiotic for each patient using a bespoke mathematical expression (see Appendix 2) that performs two functions: firstly, it weights the values for each antimicrobial characteristic from steps 1-3 using their respective weights from the discrete choice experiment ($w_c$, $w_t$, $w_u$, $w_o$, $w_i$, $w_a$, $w_r$); secondly, it incorporates an illness severity variable that weights the contribution of antibiotic susceptibility probability and intravenous administration to overall utility more heavily as it increases.

The distributions of utility values were plotted across the microsimulation dataset for each antibiotic and antibiotic combination using a box and whisker plot. Sensitivity analyses where the utility distribution was repeatedly calculated on the microsimulation dataset across a range of zero to one for susceptibility probability, CDI probability, toxicity probability and normalised cost, and a range of minus two to two for characteristic weights - changes in overall utility distribution in the microsimulation dataset across these ranges were then plotted for all antibiotics using line plots.

## Microsimulation study design

The single antibiotic with the highest utility value for each patient in the microsimulation dataset was selected as the personalised first-line treatment recommendation for that patient. The antibiotic with the second-highest utility was chosen as second-line option, and so on until all 13 antimicrobial agents were ranked as treatment recommendations in order of utility value for each patient. Sensitivity analyses were performed to assess the effect of increasing the illness severity weight from zero up to 30 on the proportion of subsequent isolates that were covered by all recommendations, Access-category recommendations, recommendations with oral routes of administration, and recommendations with intravenous routes of administration - these results were plotted using area and line plots respectively. Proportions were compared visually against a standard first-line option for systemically well patients (nitrofurantoin) and for critically ill patients (piperacillin-tazobactam) - these agents were selected as comparators because they were the oral and intravenous antibiotics respectively that had the highest rates of susceptibility in the dataset. 

The illness severity sensitivity analysis was then repeated to assess the potential effect of improved probability predictions by taking the mean of the predicted probability and the actual outcome denoted by one or zero, then again plotting outcomes and antibiotics recommended across illness severity using an area and line chart respectively. The analysis was repeated again to assess the effect of increased illness severity on the predicted CDI risk, toxicity risk, and minimum financial cost of first-line treatment recommendations. A subanalysis was also performed in inpatients on antibiotic therapy at the time of urine sampling to compare recommended antibiotics against those that were actually prescribed. A similar sensitivity analysis was performed to assess the effect of increasing nitrofurantoin resistance probability from zero up to one on the four outcomes listed above and antibiotics recommenced, plotting the results using an area and line chart respectively.

# Results

## Participants

The characteristics of patients, antimicrobial agents, and specimens in the model development and microsimulation datasets are summarised in Appendix 2. The proportion of all patient/specimen characteristics between the urine model development dataset and microsimulation dataset were comparable. A higher proportion specimens in both urine datasets came from females than males, while the number of prescriptions for all indications in the prescription dataset was approximately equal between males and females. 60-69 was the most prevalent age group in the prescription dataset, while 70-79 was the most prevalent age group in both urine datasets. A higher proportion of patients in the prescription dataset had English recorded as spoken language than in either urine dataset - this probably reflects the fact that the prescription dataset pertained only to inpatients (where more detailed data collection may have been performed), while the urine dataset pertained to both inpatients and outpatients. The numbers of different antibiotic types prescribed in the prescription dataset are summarised in Appendix 3.

## Clinical prediction model specification and performance

Final training hyperparameters used for each model are summarised in Appendix 4, and predictor variable contributions to predictive performance are summarised in Appendix 5. The presence and/or absence of previously-detected resistance to an antibiotic in the last year was one of the largest contributors to predictive accuracy for susceptibility to that antibiotic in most models. Gender and age group were large contributors to antibiotic susceptibility predictive performance in all models. Hospital admission in the last year, age over 65, previous CDI, and previous acute kidney injury in the last year were among the largest contributors to predictive performance for both CDI and antibiotic toxicity.

ROC curves and performance metrics for models predicting single-antibiotic suceptibility, CDI, and toxicity in the single validation run on the validation dataset are summarised in Figure 1 and Table 1 respectively. ROC curves and performance metrics for models predicting susceptibility to 2-antibiotic combinations are summarised in Appendix 6 and Appendix 7 respectively.

```{r, echo=FALSE,message=FALSE,warning=FALSE}

knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/roc_plots_main.pdf")
  
```
Figure 1: ROC curves for probability predictions made for single-antibiotic susceptibility, CDI, and toxicity on the validation dataset.

```{r, echo=FALSE,message=FALSE,warning=FALSE}

matrics_singles_table <- read_csv("/Users/alexhoward/Documents/Projects/UDAST_code/metrics_singles_table.csv")

knitr::kable(matrics_singles_table)

```
Table 1: Model performance metrics for prediction of single-antibiotic susceptibility, CDI, and antibiotic toxicity in the single validation run on the validation dataset.

Performance metrics from the small training dataset stability analysis are summarised in Appendix 8. The largest difference in mean AUROC across six model train-test iterations was observed between the 2% and 12% training datasets in the CDI prediction model (0.132, 0.631 versus 0.763), and the largest AUROC standard deviation was observed in the 2% dataset in the CDI prediction model (0.025). Performance metrics from the year group cluster analyses are summarised in Appendix 9. For all models and all year groups, mean AUROC across six train-test iterations was highest in the same year group upon which the model had been trained. The largest difference in AUROC between year groups was observed for the toxicity prediction model between 2011-2013 training, 2011-2013 testing and 2011-2013 training, 2014-2016 testing (0.257, 0.880 versus 0.623). The largest AUROC standard deviation was observed for the CDI model with 2008-2010 training, 2011-2013 testing (0.091).

Performance metrics from the model fairness analyses are summarised in Appendix 10. The largest difference in mean AUROC across six model train-test iterations in the age fairness analysis was observed between the 40-49 and 80-89 age groups for piperacillin-tazobactam susceptibility prediction (0.136, 0.831 versus 0.695), and the largest AUROC standard deviation was observed for CDI prediction in the over 90s age group (0.077). The largest difference in mean AUROC across six model training iterations in the race fairness analysis was observed between hispanic patients and the group containing the least prevalent local races (i.e., not Asian, black, or white) for ciprofloxacin prediction (0.093, 0.738 versus 0.646), and the largest AUROC standard deviation was observed in Asian patients for CDI prediction (0.078). The largest difference in mean AUROC across six model training iterations in the remaining characteristics was observed between females and males - this difference was most marked in susceptibility prediction models for cephalosporins, meropenem, and gentamicin, with the largest difference in mean AUROC observed for meropenem (0.102, 0.727 versus 0.625), and the largest AUROC standard deviation was observed in piperacillin-tazobactam susceptibility prediction for males (0.023).

## Discrete choice experiment

49 prescribers participated in the discrete choice experiment (15 from infection specialties, 11 from other medical specialties, nine from surgical specialties, nine from intensive care, and five from general practice). The coefficients from the ranked logit analysis of the discrete choice experiment across all specialties are displayed in Figure 3, and the coefficients by individual specialty are displayed in Appendix 11. Overall, an antibiotic being specific to urinary tract infection in its licensed indication appeared to have the most positive effect on its probability of selection in the discrete choice experiment, and high toxicity risk the greatest negative effect on its probability of selection. High antibiotic cost appeared to have the least effect on probability of antibiotic selection, exhibiting a minimal positive effect with the approximated 95% confidence interval crossing the line of no effect.

```{r, echo=FALSE,message=FALSE,warning=FALSE}

knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/ORplot.pdf")

```
Figure 2: Coefficients from the ranked logit analysis of the discrete choice experiment across all prescriber specialties

General practitioners and infection specialists were more likely to select antibiotics with oral routes of adminstration, and less likely to select antibiotics with intravenous routes of administration - the opposite was observed in intensive care and medical specialty prescribers, where intravenous routes of administration were preferred over oral routes. Routes of administration appeared to have the least bearing on choice in surgical specialties. In medical specialties and intensive care, an antibiotic being in the WHO Access category most increased the probability of selection. Intensive care was the only specialty in which an antibiotic being in the WHO Reserve category reduced the probability of selection more than high toxicity risk.

## Utility function

The distributions of single-antimicrobial choice utility values for patients across the microsimulation dataset at a default illness severity value of one are displayed in Figure 2, The distributions of single-agent utility values stratified by prescriber types are displayed in Appendix 12, and the distribution of all antibiotic options including 2-agent combinations are displayed in Appendix 13.

```{r, echo=FALSE,message=FALSE,warning=FALSE}
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/utility_Treatment_ (single agent).pdf")
```
Figure 3: Distributions of antimicrobial choice utility values for patients across the microsimulation dataset.

At the lowest illness severity, Nitrofurantoin had the highest treatment choice utility for the majority of patients in the microsimulation dataset. For single-agent treatment, five of the six antibiotics with the highest median utility were WHO Access agents - the only exception to this was piperacillin-tazobactam, which placed fourth overall amongst single agents. The lowest-placed WHO Access antibiotic was ampicillin (or amoxicillin), which placed 12th out of 13 agents. Vancomycin was the lowest-placed antibiotic choice for the majority of patients. Nitrofurantoin was the highest-placed agent for all prescriber specialties, and the placement of other agents was more variable. The largest differences observed between antibiotic utility values were between general practice and intensive care - agents with oral routes of administration ranked higher in the former, while those with intravenous routes of administration ranked higher in the latter. When two-agent combinations were included in the general analysis, ampicillin plus gentamicin became the second-placed treatment option (behind nitrofurantoin), meropenem plus vancomycin the third, and piperacillin-tazobactam plus ciprofloxacin the fourth.

The sensitivity analyses that examined the effect of varying different utility function parameters on overall treatment choice utility are displayed in Appendix 14.

Describe which agents have the steepest gradients across each variable, and explain what has caused that (e.g., the agent with the highest overall toxicity probability causing the steepest gradient for toxicity risk)

Report the sensitivity analysis (line charts varying weights) for the utility function.

## Microsimulation

Report the results of the microsimulation study.

# Discussion

## Interpretation

Give an overall interpretation of the main results, including issues of fairness in the context of the objectives and previous studies

## Limitations

Discuss any limitations of the study (such as a non-representative sample, sample size, overfitting, missing data) and their effects on any biases, statistical uncertainty, and generalisability

## Usability of the model in the context of current care

Describe how poor quality or unavailable input data (eg, predictor values) should be assessed and handled when implementing the prediction model

Specify whether users will be required to interact in the handling of the input data or use of the model, and what level of expertise is required of users

Discuss any next steps for future research, with a specific view to applicability and generalisability of the model

# References

::: {#refs}
:::

# Open science

## Funding

This research was funded in part by the Wellcome Trust grant ref: 226691/Z/22/Z. For the purpose of open access, the author has applied a CC BY public copyright licence to any Author Accepted Manuscript version arising from this submission. Office for Life Sciences Data-Action Accelerator award also supported this work. The funders had no role in the conceptualisation, design, data collection, analysis, decision to publish or preparation of the manuscript.

# Acknowledgements

## Conflicts of interest

Alex Howard declares personal consulting work for Pfizer outside the submitted work, and a donation from Pfizer to the University of Liverpool for a public and professional engagement project outside the submitted work.

## Protocol

Indicate where the study protocol can be accessed or state that a protocol was not prepared

## Registration

Provide registration information for the study, including register name and registration number, or state that the study was not registered

## Data sharing

The MIMIC-IV version 2.2 data set is publicly accessible as a credentialed PhysioNet user at https://physionet.org/content/mimiciv/2.2/ once mandated training is completed, and the data use agreement is signed. Additional aggregate-level data can be provided by the authors if requests to do so are in line with legal and ethical data use regulations. Open-source code written for this study will be made available upon publication.

## Code sharing

Provide details of the availability of the analytical code

## Contributions

AH conceived the study, performed data engineering and mathematical modelling, and wrote the manuscript including diagrams.

## Patient and public involvement

Provide details of any patient and public involvement during the design, conduct, reporting, interpretation, or dissemination of the study or state no involvement

