---
title: "Automating antibiotic treatment choice using clinical prediction modelling: a microsimulation study"
format:
  docx:
    reference-doc: doc_template.docx
bibliography: UF_references.bib
csl: /Users/alexhoward/Zotero/styles/the-lancet-infectious-diseases.csl
editor: visual
---

Alex Howard\*\
\*corresponding author alexander.howard\@liverpool.ac.uk\
\*\*joint senior author

# Summary

## Background

Antibiotic prescribing is a key driver of antimicrobial resistance. Machine learning can help predict antimicrobial resistance – to enable AI-driven personalised antibiotic decision making that is implementable in real-world settings, these predictions need to be tied to a measure of drug value (or utility) that reflects real-world clinical priorities and evolving antimicrobial resistance patterns. Here, we used the results of a structured antibiotic ranking tool undertaken by real-world prescribers to develop and test utility functions that facilitate the development of artificial intelligence algorithms for directing antibiotic treatment and antimicrobial susceptibility testing – the aim of this approach was to maximise the projected benefits and practicality of antibiotic treatment for urinary tract infection while minimising harm and generation of antimicrobial resistance.

## Methods

Logistic regression models were trained, tested and validated to predict the risk of antimicrobial resistance, sepsis adverse outcomes, Clostridioides difficile diarrhoea, and drug toxicity for a range of antimicrobial agents in patients with culture-positive urine specimens using open-source real-world pseudonymised electronic healthcare records. Prescribers from a range of primary and secondary care settings undertook a ranking exercise with fictional antimicrobial agents – a ranked logit method was used to determine the relative importance of different antimicrobial characteristics (e.g., oral bioavailability, toxicity, antimicrobial resistance risk) to prescribers. Two utility functions were written that combined these importance weights with probabilities from the logistic regression models to produce an antimicrobial utility value for each patient – one for antimicrobial susceptibility testing, another for antimicrobial treatment. These utility values were then used to provide recommendations for urinary tract infection antimicrobial treatment and susceptibility testing in a microsimulation study design – the antimicrobial choices, resistance patterns, available routes of administration, and World Health Organisation Access, Watch, Reserve category of these recommendations were compared against a standard formulary treatment approach. Sensitivity analyses were performed to examine the performance of the utility function across a range of parameters and clinical settings.

## Findings

2,646 patients who had a urine specimen sent between 2009 and 2018 were assessed in the microsimulation study. The treatment utility function recommended nitrofurantoin as first-line treatment in 92.6% (n=2,451) of cases, a WHO Access agent in 99.8% (n=2,642) of cases, an orally-bioavailable agent in 98.2% (n=2,599) of cases, and an intravenously-administrable agent in 7.3% (n=194) of cases, resulting in coverage of 72.4% (n=1,915) of urine isolates that were subsequently grown. Increasing the importance weight of predicted resistance caused the utility function to recommend piperacillin-tazobactam as first-line treatment in 59.6% (n=1,733) of cases, a WHO Access category agent in 27.7% (n=733) of cases, an orally-bioavailable agent in 30.7% (n=813) of cases, and an intravenously-administrable agent in 93.4% (n=2,471) of cases, resulting in coverage of 87.5% (n=2,316) of urine isolates subsequently grown. Simulating an increase in nitrofurantoin resistance rates to 99.3% caused the utility function to recommend trimethoprim-sulfamethoxazole as first-line treatment in 48.6% (n=1,286) of cases, a WHO Access category agent in 99.9% (n=2,643) of cases, an orally-bioavailable agent in 96.1% (n=2,544) of cases, and an intravenously-administrable agent in 100.0% (n=2,646) of cases, resulting in coverage of 64.0% (n=1,693) of urine isolates subsequently grown. On average, the antimicrobial susceptibility testing utility function provided one more susceptible result per 6-agent panel for WHO Access category agents (median 4 \[IQR 3-5\] versus median 3 \[IQR 1-3\]) than a standard panel approach, provided at least one susceptible result for an Access agent in 95.9% (n=2538) of cases, at least one susceptible result of any kind in 97.1% (n=2,569) of cases, and a result of any kind for the antimicrobial agent the patient was currently prescribed in 72.8% (n=485) of prescriptions.

## Interpretation

Our prescribed-derived antimicrobial utility functions could enable true AI-driven decision making in antimicrobial treatment and susceptibility testing and help nations meet the United Nations target of 70% WHO Access agent prescribing without compromising on treatment efficacy. The prescriber-derived approach is flexible enough to be adaptable to a range of healthcare settings by allowing local clinician factors such as cost and patient cohort to influence the weights of different factors, as well as local epidemiology of resistance, toxicity, C difficile, and sepsis outcomes. Strengthening the prediction model by incorporating more healthcare data from local settings is likely to further improve performance.

## Funding

This research was funded in part by the Wellcome Trust grant ref: 226691/Z/22/Z. For the purpose of open access, the author has applied a CC BY public copyright licence to any Author Accepted Manuscript version arising from this submission. Office for Life Sciences Data-Action Accelerator award also supported this work. The funders had no role in the conceptualisation, design, data collection, analysis, decision to publish or preparation of the manuscript.

# Introduction

## Background

Functioning antibiotics underpin functioning healthcare systems - antimicrobial resistance (AMR) is therefore a significant threat to the delivery of global healthcare. In September 2024, the United Nations General Assembly (UNGA) committed to ensuring that the safest, cheapest antibiotics with the lowest AMR risk (World Health Organisation \[WHO\] Access Category antibiotics) make up 70% of global antibiotic use in healthcare by 2030. The challenge in achieving this goal is determining when it is safe to empirically use Access agents, which often have relatively narrow spectra of activity and will therefore be avoided in favour of more harmful Watch category agents to ensure treatment efficacy where there is doubt about the presence of resistance and/or perceived high stakes (e.g., systemically unwell patients).

Automating antimicrobial treatment choice using machine learning is therefore attractive because algorithms can better quantify the probability of resistance and treatment outcomes using clinical prediction models built on drug, patient, and pathogen characteristics. Algorithms, however, cannot judge the subjective value (or cost) of treatment outcomes to patients, populations, and healthcare systems. To make good antibiotic treatment decisions, algorithms need a mechanism to quantify the value of antibiotic treatment mathematically - a so-called utility function. An automated system that makes antibiotic treatment choices using such a utility function could give prescribers the confidence to use Access category antibiotics when safe to do so, while compromising on Access antibiotic use in situations where treatment efficacy needs to be prioritised (e.g., where risk of resistance and/or clinical deterioration from untreated infection is high).

Here, we describe a utility function that combines clinical prediction models with prescriber-informed outcome values/costs to automate the process of antibiotic treatment choice for urinary tract infection (UTI) to help meet the UNGA 70% global Access antibiotic target without compromising individual patient safety.

# Methods

## Data Sources and Participants

The study complied with the MIMIC-IV dataset Data Use Agreement 1.5.0 and Health Data License. Approval was also obtained for the discrete choice experiment from the NHS Health Research Authority (HRA). A proportionate review was undertaken by a UK Research Ethics Committee (REC), which determined that full REC approval was not required. Clinical prediction models were developed using the open-source PhysioNet dataset MIMIC (Medical Information Mart for Intensive Care)-IV version 2.2, a pseudonymised inpatient and outpatient electronic healthcare record dataset for patients over the age of 18 who were admitted to intensive care or the emergency department between 2008 and 2019 at Beth Israel Deaconess Medical Center (BIDMC - Boston, MA) (<https://physionet.org/content/mimiciv/2.2/>).

Prediction models of antibiotic resistance were developed on urine specimen results with bacterial growth from the `microbiologyevents` dataset (inpatients and outpatients), and prediction models of antibiotic outcomes were made using prescription data from the `prescriptions` dataset (inpatients only). Antimicrobial ssuceptibility interpretations in the microbiologyevents dataset are likely to be based on Clinical Laboratory Standards Institute clinical breakpoints given the U.S. setting \[ref\]. Sample size was determined by the size of the available dataset, and provided a sufficient number of cases per independent variable in the model to reduce the probability of overfitting \[ref\]. Antibiotic financial cost was determined using the lowest procurement/tender award price for each agent in U.S. dollars as listed on the U.S. Department of Veterans Affairs National Acquisition Center (<https://www.vendorportal.ecms.va.gov/NAC/>) when accessed in November 2024.

Antibiotic treatment value was determined using a discrete choice experiment survey completed online between June 1st 2024 and October 31st 2024 by antibiotic prescribers from general practice (GP), pharmacy, medicine, surgery, intensive care, and infectious diseases/medical microbiology working for healthcare providers serviced by diagnostic laboratories at Liverpool University Hospitals NHS Foundation Trust (Liverpool Clinical Laboratories). Sample size was determined by the number of respondents in the 4-month survey period, and provided a sufficient number of cases per independent variable in the model to reduce the probability of overfitting \[ref\].

## Data preparation

Data preprocessing and quality checking was performed in a similar way across sociodemographic groups using RStudio version 2023.12.1+402, and followed a similar pattern to our previous work with the dataset with some additions \[ref\]. The preprocessing workflow is summarised in Appendix x. Data were quality checked at each stage throughout preprocessing using counts of organism species, results, antimicrobial names, missing data, numbers of rows and numbers of columns to detect preprocessing errors. Choices of predictor variables and their time horizons were based on indirect or direct causal plausibility and/or association with outcome variables \[ref\]. The predictor variables used in each model are listed in Appendix x.

39 outcomes were required for 39 individual models - 13 individual antibiotic susceptibility prediction models and 24 antibiotic combination resistance models on the urines dataset, and a CDI prediction model and antibiotic toxicity prediction model on the prescriptions dataset. The outcome for all antibiotic susceptibility prediction models was an 'S' result indicating susceptibility of the organism grown in that urine specimen to that antibiotic or antibiotic combination. The outcome for a *Clostridioides difficile* infection (CDI) prediction model was a positive *C. difficile* stool result within 3 months of the start date/time of an antibiotic or antibiotic combination. The outcome for an antibiotic toxicity prediction model was a composite outcome of either stage 3 acute kidney injury (a new increase in serum creatinine to at least three times baseline or at least 3.54µmol/dL in the absence of co-administration of nephrotoxic drugs as defined by the British National Formulary \[ref\] or intravenous contrast during the associated hospital admission), deranged liver function tests (a result newly above the upper end of the normal range for alanine aminotransferase, aspartate aminotransferase, or alkaline phosphatase in the absence of previous coded chronic liver disease or biliary instrumentation in the associated hospital admission), marrow suppression (new anaemia, leukopenia, or thrombocytopenia in the absence of co-administration of cytotoxic drugs as defined by the British National Formulary \[ref\]) or coded bleeding diagnosis in the associated hospital admission) in the seven days following the start date of an antibiotic or a coded antibiotic adverse event for the associated hospital admission. The selection process for predictor and outcome variables was undertaken by the lead author (AH \[Consultant in Medical Microbiology, male, 30s, white British\]) and reviewed by medically-qualified co-authors (CB \[Consultant in Medical Microbiology, female, 30s, white British\], AG \[Consultant in Medical Microbiology, male, 30s, Maltese\], WH \[Consultant in Medical Microbiology, male, 50s, white Australian\], IB \[Consultant in Public Health, male, 50s, white British\]). Outcome and predictor variables were selected consistently across sociodemographic groups. No blinding to allocation or predictor/outcome assessment was implemented at any stage.

## Clinical prediction models

We have described clinical prediction models of resistance for 12 of the 13 antimicrobial agents using logistic regression in our previous work \[ref\]. For this study, XGBoost (an ensemble method that improves predictive accuracy by sequentially combining multiple decision trees) was used instead because it was found to achieve higher initial area under the receiver operating characteristic curve (AUROC) values for the majority of the 12 clinical prediction models when trained and validated on the same datasets. The same XGBoost technique was used for all 39 models. The output of each model was the probability of the binary outcome's positive class (an 'S' result, CDI, or antibiotic toxicity).

XGBoost models were trained and tuned on the training datasets using L2 regularisation (ridge penalty) to control overfitting. AUROC was used as the model evaluation metric. Hyperparameter tuning was performed sequentially for each of the 39 models across parameters in 3 stages. 10 sets of paired values for maximum tree depth (selecting values between two and nine inclusive) and minimum child weight (selecting values between one and 10 inclusive) were randomly selected using latin hypercube sampling with `randomLHS()` from the `lhs` package \[ref\] - a 5-fold cross-validation was then performed across 50 boosting rounds for each of these 10 pairs of values, with a default learning rate of 0.05, a proportion of cases used for each decision tree (subsample rows ratio) of 0.8, and a proportion of predictors used for each decision tree (subsample columns ratio) of 0.8. The maximum tree depth and minimum child weight values that produced the highest AUROC were carried forward to another set of 5-fold cross-validations with 50 boosting rounds across 10 paired subsample rows and columns ratios (again randomly selected using `randomLHS`, with both values sampling from a range between 0.5 and 1 inclusive) with a learning rate of 0.05. The subsample rows and columns ratios that produced the highest AUROC were carried forward to a final set of 5-fold cross-validations across four learning rates (0.1, 0.05, 0.01, and 0.001), with a maximum number of boosting rounds of 1000 and early stopping if AUROC did not improve for 50 rounds. The learning rate and the round number at which the best AUROC value occurred (forming the new total number of rounds parameter) were then taken forward as parameters for training the final model. Class imbalance methods (e.g., class weighting) were not used to avoid inflating the probability of rare occurrences (e.g., CDI) that would subsequently be used to inform the utility function. The final model parameters for each model are listed in Appendix x.

Predictor variable contributions to predictive value in model training were measured using Shapley values with `predict(predcontrib=TRUE)` from the `stats` package \[ref\]. Predictor variables with Shapley values of zero were then excluded from models for the final training round. A single validation run was then performed with the final model on the validation dataset, measuring AUROC, accuracy, precision, recall, and F1 score (decision threshold 0.5) using coded formulae for each model. A model stability analysis was performed for each model excluding the 24 antibiotic combination models (15 models in total) in view of the computational time required, where training and validation of the final model was performed in six random train-test splits without replacement for each four smaller train-test dataset size ratios (2:98%, 6:94%, 8:92%, and 12:88%) - AUC, accuracy, precision, recall, and F1 score were measured for each of these validations, and metric distributions were plotted using dot plots to assess heterogeneity in performance. A model fairness analysis was performed where each of the same 15 trained models was validated separately across a range of protected characteristics (race, age, marital status, first language, and gender), with six random-train-test splits per characteristic - AUC, accuracy, precision, recall, and F1 score (decision threshold 0.5) were again measured for each of these validations, and metric distributions plotted using dot plots. A time cluster analysis was performed to assess out-of-sample performance, where the 15 models were trained on one of four time periods (2008-2010, 2011-2013, 2014-16, and 2017-2019), then validated on that period and the other four time periods, measuring AUC, accuracy, precision, recall, and F1 score (decision threshold 0.5) - this was repeated with six random train-test splits for each pair of time periods, and metrics distributions were plotted using dot plots.

## Prescriber discrete choice experiment

A questionnaire was devised (see Figure x) where 13 fictional antibiotics were created and given six characteristics: the AWaRe classification of the antibiotic (yes or no); the CDI risk of the antibiotic (high or low); the toxicity risk of the antibiotic (high or low); whether the antibiotic was only indicated for UTI (Yes or no); whether the antibiotic was orally-administrable (yes or no); whether the antibiotic was intravenously-administrable (yes or no); the financial cost of the antibiotic (high or low). Participants were given a fictional UTI clinical scenario, then asked to rank the treatments in order of suitability. The questionnaire was distributed to participants using SurveyMonkey (<https://uk.surveymonkey.com>) over a 4-month period (1st June 2024 to 21st October 2024) via single organisational and departmental points of contact in infection specialties, medicine, surgery, intensive care, and general practice by email invitation, and written consent for participation was obtained. No minimum sample size was stipulated as over-fitting was not a concern (the approach is designed for the questionnaire to be distributed in the same population in which the algorithm would be used), but the aim was to obtain at least some participants from each specialty.

The ranking of each antibiotic (1-13) by each participant was recorded and converted to a long format where each participant's chosen rank order of antibiotics was treated as a series of choices using `mlogit.data(ranked=TRUE)` from the `mlogit` package \[ref\]. A conditional logit model was built to estimate the probabilities of each observed rank from the choice data based on the six characteristics of each antibiotic (again using `mlogit`). The model was trained to estimate maximum log likelihood using the BFGS method \[ref\]. The coefficients for each characteristic were then extracted from the model and used to represent the relative weight of each antimicrobial characteristic to the expert participants in making an antimicrobial treatment choice for UTI. Coefficients were analysed using data visualisation by bar plot. A sensitivity analysis was performed where the approach was repeated on subsets of the data to build separate models for infection specialties, medicine, surgery, intensive care, and general practice to compare the weights of the six antimicrobial characteristics to different specialties.

## Utility calculation

Treatment utility was calculated individually for each patient/urine specimen in the microsimulation dataset using the following process:

1.  The trained clinical prediction models were used to predict the probability of antimicrobial susceptibility, producing values between zero and one for ($v_s$), CDI ($v_c$), and toxicity ($v_t$) for each antibiotic (and antibiotic combination) on each urine specimen. These values were then weighted by their respective AUROC values on the test dataset ($a_s$, $a_c$, and $a_t$) to reflect confidence in the probability estimates.

2.  Probabilities were added for each known binary characteristic of antimicrobials - UTI-specificity ($v_u$), oral administration option ($v_o$), and intravenous administration option ($v_i$), with a probability of one for yes and zero for no. Separate binary variables were created for Access ($v_a$) and Reserve ($v_r$) categories, also with a probability of one for yes and zero for no (watch category agents were zero for both).

3.  Financial cost was calculated for each antibiotic using the lowest available cost of an antibiotic in U.S. dollars as listed on the U.S. Department of Veterans Affairs National Acquisition Center - these costs were divided by the highest cost including combinations of antibiotics, producing normalised costs between 0 and 1 ($v_h$).

4.  A personalised utility value was calculated for each antibiotic for each patient using a bespoke mathematical expression (see Appendix x) that performs two functions: firstly, it weights the values for each antimicrobial characteristic from steps 1-3 using their respective weights from the discrete choice experiment ($w_c$, $w_t$, $w_u$, $w_o$, $w_i$, $w_a$, $w_r$); secondly, it incorporates an illness severity variable that weights the contribution of antibiotic susceptibility probability and intravenous administration to overall utility more heavily as it increases.

The distributions of utility values were plotted across the microsimulation dataset for each antibiotic and antibiotic combination using a box and whisker plot. Sensitivity analyses where the utility distribution was repeatedly calculated on the microsimulation dataset across a range of zero to one for susceptibility probability, CDI probability, toxicity probability and normalised cost, and a range of minus two to two for characteristic weights - changes in overall utility distribution in the microsimulation dataset across these ranges were then plotted for all antibiotics using line plots.

## Microsimulation study design

The single antibiotic with the highest utility value for each patient in the microsimulation dataset was selected as the personalised first-line treatment recommendation for that patient. The antibiotic with the second-highest utility was chosen as second-line option, and so on until all 13 antimicrobial agents were ranked as treatment recommendations in order of utility value for each patient. The outcomes of interest were the following, measured at illness severity weights of zero (representing systemically well patients) and 30 (representing critically unwell patients):

1.  The proportion of organisms grown in urine that were susceptible to the recommended first-line antibiotic

2.  The proportion of organisms grown in urine that were susceptible to a recommended Access category first-line antibiotic

3.  The proportion of organisms grown in urine that were susceptible to a recommended first-line antibiotic with an oral route of administration

4.  The proportion of organisms grown in urine that were susceptible to a recommended first-line antibiotic with an intravenous route of administration

Proportions were compared against a standard first-line option for systemically well patients (nitrofurantoin) and for critically ill patients (piperacillin-tazobactam) - these agents were selected as comparators because they were the oral and intravenous antibiotics respectively that had the highest rates of susceptibility in the dataset. A chi-squared test was used as a significance test for the comparisons with a significance threshold of 5% – results are reported as proportions for the two groups with 95% confidence intervals. Sensitivity analyses were performed to assess the effect of increasing the illness severity weight from zero up to 30 on the four outcomes listed above (for first-line single antibiotics, then for two-agent combinations, then for second-line antibiotics) and the antibiotics recommended - these results were plotted using area and line plots respectively. This analysis was then repeated to assess the potential effect of improved probability predictions by taking the mean of the predicted probability and the actual outcome denoted by one or zero, then again plotting outcomes and antibiotics recommended across illness severity using an area and line chart respectively. A similar sensitivity analysis was then performed to assess the effect of increasing nitrofurantoin resistance probability from zero up to one on the four outcomes listed above and antibiotics resommenced, plotting the results using an area and line chart respectively.

## Analytical methods

Describe how the data were used (eg, for development and evaluation of model performance) in the analysis, including whether the data were partitioned, considering any sample size requirements

Describe any model updating (eg, recalibration) arising from the model evaluation, either overall or for particular sociodemographic groups or settings

## Ethical approval

## Setting and ethics

## Participants

## Descriptive data and statistical modelling

R version 4.3.2 (<https://cran.r-project.org>) was used for data cleaning, the majority of statistical analyses/data visualisation, and clinical prediction model development for *C difficile*, drug toxicity, and adverse outcomes from sepsis. AMR package version 2.1.1 was used to standardise organism and susceptibility data. \[package used for clinical prediction models\]. Python version 3.12.0 (<https://www.python.org/downloads/release/python-3120/>) was used for developing AMR clinical prediction models (building on approaches from our previous work - Nature Comms ref) and the remaining statistical analyses/data visualisation.

156 AMR clinical prediction models were trained, tested, and validated - one model for each antimicrobial agent, and one model for each possible combination of antimicrobial agents. The model development data were randomly split stratified by the outcome variable into subsets for training and hyperparameter optimisation (80% of the data) and validation (20% of the data). Ten iterations of k-fold cross-validation (six folds) with random shuffling were used to fit models, select features, and tune hyperparameters. The final selected model for each of the 156 antimicrobial combinations was validated once.

Three binary logistic regression clinical prediction models were also developed based on linked antimicrobial prescription data for patients in the model development dataset - these models predicted the probability of a positive *C difficile* stool result in the 3 months after antimicrobial treatment, the probability of renal, haematological, or hepatic derangement (representing possible drug toxicity) in the seven days following antimicrobial treatment, and the probability of sepsis with a hospital stay of more than seven days, intensive care admission, or death in the 28 days (representing possible adverse outcomes from sepsis) following antimicrobial treatment. The *C difficile* and toxicity models included the specific agent used as a predictor variable, and the sepsis adverse outcomes model did not.

## Clinical prediction model statistics and reproducibility

Scikit-Learn package version 1.4.1 (<https://scikit-learn.org/stable/>) and Mlxtend version 0.23.1 13 were used to train, test, and validate the 156 AMR binary logistic regression models. Models were trained using maximum likelihood estimation, with susceptible results as positives and resistant results as negatives. The model predicted the probability of a susceptibile result for each urine specimen. Regularisation using a least absolute shrinkage and selection operator (LASSO, or L1) was used to prevent overfitting and remove predictor variables that did not improve predictions. Hyperparameter tuning with grid search cross-validation controlled strength of regularisation to optimise AUC-ROC. Model performance on the validation data was assessed using the scikit-learn roc_auc_score function and classification_report methods to calculate AUC-ROC, precision, recall, accuracy, and 50% decision-threshold f1-score for both susceptible and resistant classes.

## Microsimulation study design

Figure 8 summarises the study design for the microsimulation (individual patient-level simulation) study.

**Figure 8**: **Design of the microsimulation study.** For each specimen in the microsimulation study dataset, a personalised panel was composed using prediction modelling and a test prioritisation function to maximise the probability of susceptible (‘S’) results for Access agents, and failing that, susceptible results for other agents – the results for agents in this panel were then populated by actual results for those agents in the dataset. The number of susceptible results for Access agents and all agents that would have been provided by using this panel was then compared against a standard panel based on international UTI treatment guidelines. Created in BioRender. Howard, A. (2024) BioRender.com/k67g601.

The microsimulation study was performed by interfacing R version 4.3.2 and Python version 3.12.0 using the R package reticulate version 1.36.0 (<https://cran.r-project.org/web/packages/reticulate/index.html>).^25^ For each specimen, six of the 12 agents were selected as the simulated personalised testing panel, by prioritising based on probability of susceptibility – WHO Access agents with \>50% probability of susceptibility were tested automatically, then the rest of the six-agent testing panel was filled with remaining agents that had the highest probabilities of susceptibility (regardless of whether they were Access or Watch category agents). The personalised panel for each specimen was then compared with a standard non-personalised testing panel composed of four antimicrobial agents recommended for UTI by the WHO essential medicines list (nitrofurantoin, trimethoprim/sulfamethoxazole, ceftriaxone, and ciprofloxacin) and two agents recommended by European Association of Urology guidelines (gentamicin and piperacillin/tazobactam).^26,27^

## Outcome variables

The outcomes of interest were:

·      The number of susceptible results per specimen six-agent panel

·      The number of susceptible results for Access agents per specimen six-agent panel

·      The proportion of specimen six-agent panels with no susceptible results

·      The proportion of specimen six-agent panels with no susceptible results for Access agents

## Microsimulation study statistics and reproducibility

Numbers of susceptible results per panel were compared between the personalised and standard fixed panel on each specimen using a Wilcoxon two-sample paired signed rank test with continuity correction – results are presented as effect size (calculated by dividing the test’s Z statistic by the square root of the number of specimens).^28^ Medians and interquartile ranges of the number of susceptible results per panel were calculated for data visualisation purposes. Proportions of specimens with panels with at least one susceptible result were compared using a chi-squared test – results are reported as proportions for the two groups and 95% confidence interval. For both analyses, a significance threshold of 5% was used. A sensitivity analysis was also performed for the test prioritisation function, where the analysis was repeated across nine different susceptibility probability thresholds for automatic inclusion of Access agents on the personalised panel (from \> 10% probability of susceptibility to \> 90%). This sensitivity analysis was then repeated after also repeating the modelling and panel selection steps, but with all intermediate (I) results classified as resistant instead of susceptible.

# Results

## Participants

Describe the flow of participants through the study, including the number of participants with and without the outcome and, if applicable, a summary of the follow-up time. A diagram may be helpful

Report the characteristics overall and, where applicable, for each data source or setting, including the key dates, key predictors (including demographics), treatments received, sample size, number of outcome events, follow-up time, and amount of missing data. A table may be helpful. Report any differences across key demographic groups

Identify any differences between the development and evaluation data in healthcare setting, eligibility criteria, outcome, and predictors

For model evaluation, show a comparison with the development data of the distribution of important predictors (demographics, predictors, and outcome)

## Model development

Specify the number of participants and outcome events in each analysis (eg, for model development, hyperparameter tuning, model evaluation)

## Model specification

Provide details of the full prediction model (eg, formula, code, object, application programming interface) to allow predictions in new individuals and to enable third party evaluation and implementation, including any restrictions to access or reuse (eg, freely available, proprietary)¶

## Model performance

Report model performance estimates with confidence intervals, including for any key subgroups (eg, sociodemographic). Consider plots to aid presentation

If examined, report results of any heterogeneity in model performance across clusters. See TRIPOD-Cluster for additional details

## Model updating

Report the results from any model updating, including the updated model and subsequent performance

# Discussion

## Interpretation

Give an overall interpretation of the main results, including issues of fairness in the context of the objectives and previous studies

## Limitations

Discuss any limitations of the study (such as a non-representative sample, sample size, overfitting, missing data) and their effects on any biases, statistical uncertainty, and generalisability

## Usability of the model in the context of current care

Describe how poor quality or unavailable input data (eg, predictor values) should be assessed and handled when implementing the prediction model

Specify whether users will be required to interact in the handling of the input data or use of the model, and what level of expertise is required of users

Discuss any next steps for future research, with a specific view to applicability and generalisability of the model

# References

::: {#refs}
:::

# Open science

## Funding

This research was funded in part by the Wellcome Trust grant ref: 226691/Z/22/Z. For the purpose of open access, the author has applied a CC BY public copyright licence to any Author Accepted Manuscript version arising from this submission. Office for Life Sciences Data-Action Accelerator award also supported this work. The funders had no role in the conceptualisation, design, data collection, analysis, decision to publish or preparation of the manuscript.

# Acknowledgements

## Conflicts of interest

Alex Howard declares personal consulting work for Pfizer outside the submitted work, and a donation from Pfizer to the University of Liverpool for a public and professional engagement project outside the submitted work.

## Protocol

Indicate where the study protocol can be accessed or state that a protocol was not prepared

## Registration

Provide registration information for the study, including register name and registration number, or state that the study was not registered

## Data sharing

The MIMIC-IV version 2.2 data set is publicly accessible as a credentialed PhysioNet user at https://physionet.org/content/mimiciv/2.2/ once mandated training is completed, and the data use agreement is signed. Additional aggregate-level data can be provided by the authors if requests to do so are in line with legal and ethical data use regulations. Open-source code written for this study will be made available upon publication.

## Code sharing

Provide details of the availability of the analytical code

## Contributions

AH conceived the study, performed data engineering and mathematical modelling, and wrote the manuscript including diagrams.

## Patient and public involvement

Provide details of any patient and public involvement during the design, conduct, reporting, interpretation, or dissemination of the study or state no involvement
