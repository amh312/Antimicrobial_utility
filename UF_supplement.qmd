---
title: "Appendix"
format:
  docx:
    reference-doc: doc_template.docx
bibliography: UF_references.bib
csl: /Users/alexhoward/Zotero/styles/the-lancet-infectious-diseases.csl
editor: visual
---

```{r, echo=FALSE,message=FALSE,warning=FALSE}

library(tidyverse)
library(knitr)
library(glue)

```

## Appendix 1: Data processing workflow

```{r, echo=FALSE,message=FALSE,warning=FALSE}
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/Data flow chart.pdf")
```

### Appendix 1: Data processing workflow. Created in BioRender. Howard, A. (2025) https://BioRender.com/r77h648
\

## Appendix 2: Discrete choice experiment

A patient has urinary tract infection. You have no other details about the patient at this stage (including disease severity). Please rank the following fictional antibiotic treatments from 1 to 13 in order of most to least preferred in this scenario by dragging and dropping options or using the arrows on the right. When finished, just click the 'Done' button at the bottom of the page.
```{r, echo=FALSE,message=FALSE,warning=FALSE}

knitr::include_graphics(c(
  "/Users/alexhoward/Documents/Projects/UDAST_code/S1.png",
  "/Users/alexhoward/Documents/Projects/UDAST_code/S2.png",
  "/Users/alexhoward/Documents/Projects/UDAST_code/S3.png",
  "/Users/alexhoward/Documents/Projects/UDAST_code/S4.png",
  "/Users/alexhoward/Documents/Projects/UDAST_code/S5.png",
  "/Users/alexhoward/Documents/Projects/UDAST_code/S6.png",
  "/Users/alexhoward/Documents/Projects/UDAST_code/S7.png",
  "/Users/alexhoward/Documents/Projects/UDAST_code/S8.png",
  "/Users/alexhoward/Documents/Projects/UDAST_code/S9.png",
  "/Users/alexhoward/Documents/Projects/UDAST_code/S10.png",
  "/Users/alexhoward/Documents/Projects/UDAST_code/S11.png",
  "/Users/alexhoward/Documents/Projects/UDAST_code/S12.png",
  "/Users/alexhoward/Documents/Projects/UDAST_code/S13.png"
                          ))
                      

```

\

## Appendix 3: Model specification and training/testing/validation details for clinical prediction models

A total of 15 individual models were required for the primary analysis (i.e., excluding antibiotic combinations). 13 individual antibiotic susceptibility prediction models to predict probability of antibiotic activity against the urinary pathogen (i.e., pathogen susceptibility to the antibiotic) were trained for each antibiotic on the MIMIC-IV urine culture dataset. A *Clostridioides difficile* infection (CDI) prediction model and an antibiotic toxicity prediction model were trained on the MIMIC-IV prescriptions dataset.

The outcome for all antibiotic susceptibility prediction models was the probability of an 'S' result indicating susceptibility of the organism grown in that urine specimen to that antibiotic or antibiotic combination. The outcome for the CDI prediction model was the probability of a positive *C. difficile* stool result within three months of the start date/time of an antibiotic or antibiotic combination. The outcome for an antibiotic toxicity prediction model was the probability of a composite outcome of either stage three acute kidney injury (a new increase in serum creatinine to at least three times baseline or at least 3.54µmol/dL in the absence of co-administration of nephrotoxic drugs as defined by the British National Formulary or intravenous contrast during the associated hospital admission), deranged liver function tests (a result newly above the upper end of the normal range for alanine aminotransferase, aspartate aminotransferase, or alkaline phosphatase in the absence of previous coded chronic liver disease or biliary instrumentation in the associated hospital admission), marrow suppression (new anaemia, leukopenia, or thrombocytopenia in the absence of co-administration of cytotoxic drugs as defined by the British National Formulary) or coded bleeding diagnosis in the associated hospital admission) in the seven days following the start date of an antibiotic, or a coded antibiotic adverse event for the associated hospital admission.[@Appendix1Interactions,@kongIncidenceCharacteristicsRisk2021, @tammaAssociationAdverseEvents2017]

Choices of predictor variables and their time horizons were based on indirect or direct causal plausibility and/or association with outcome variables -this process was undertaken by the lead author (Consultant in Medical Microbiology, male, 30s, white British) and reviewed by co-authors (all male in the age range 20-60 with a racial mix of white British, White Australian, Maltese, Indian, and Chinese).[@howardPersonalisedAntimicrobialSusceptibility2024] Outcome and predictor variables were selected consistently across sociodemographic groups. No blinding to allocation or predictor/outcome assessment was implemented at any stage. Allocation to training and validation datasets was performed individually for each model by random 80:20 split without replacement, stratified to maintain similar proportions of the outcome in the training and validation datasets.

We have described clinical prediction models of resistance for 12 of the 13 antimicrobial agents using logistic regression in our previous work.[@howardPersonalisedAntimicrobialSusceptibility2024] In an attempt to capture more non-linear relationships in the data, XGBoost (an ensemble method that improves predictive accuracy by sequentially fitting decision trees in so-called boosting rounds and combining their predictions, implemented via the ‘xgboost’ package <https://cran.r-project.org/web/packages/xgboost/index.html>), was used for this study.[@chenXgboostExtremeGradient2014, @chenXGBoostScalableTree2016] XGBoost models underwent hyperparameter tuning and training on the training datasets using L2 regularisation (ridge penalty) to control overfitting. Area under the receiver operating characteristic curve (AUROC) was used as the model evaluation metric. Class imbalance methods (e.g., class weighting) were not used.

Hyperparameter tuning was performed sequentially using AUROC for each of the 15 models across hyperparameters in three stages to reduce the number of hyperparameter combinations, and therefore computational time:

1.  Ten maximum tree depth and minimum child weight values in the range two to nine and one to ten respectively were selected using Latin hypercube sampling (LHS) with 'randomLHS' from the 'lhs' package (<https://cran.r-project.org/web/packages/lhs/index.html>) and tested using 5-fold cross validation across 50 boosting rounds (learning rate 0.05, subsample row ratio 0.8, subsample columns ratio 0.8).
2.  Ten subsample row and column ratio values in the range 0.5-1 were selected and tested using the same method as above.
3.  Three learning rates (0.01, 0.05, and 0.1) and were tested using the same method again, but with a maximum of 1,000 boosting rounds and early stopping if AUROC did not improve for 50 rounds. The boosting round number at which the best AUROC value occurred was recorded.

Predictor variable contributions to predictive value (feature importances) in model training were measured using Shapley values with 'predict' from the 'stats' package.[@Chapter53Shapley] Predictor variables with Shapley values of zero were then excluded from models for the final boosting round.

A single validation run was performed with the final trained model on the validation dataset, measuring AUROC, accuracy, precision, recall, and F1 score (decision threshold 0.5) using coded formulae for each model, with 95% confidence intervals approximated using bootstrapping. An additional analysis was performed where 24 additional susceptibility prediction models were trained and validated for 2-antibiotic combinations that were used at least 500 times in the prescriptions dataset.

A model stability analysis was performed to assess model performance when trained on a smaller training dataset for each model excluding the 24 antibiotic combination models (15 models in total) in view of the computational time required. Training and validation of the final model was performed in six random train-test splits without replacement (stratified by outcome) for each of four smaller train-test dataset size ratios (2:98%, 6:94%, 8:92%, and 12:88%) - AUROC, accuracy, precision, recall, and F1 score were measured for each of these validations, and metric distributions were plotted using dot plots to assess heterogeneity in performance.

A model fairness analysis was performed where each of the same 15 trained models was validated separately across a range of protected characteristics (race, age, marital status, first language, and gender), with six random-train-test splits per characteristic - AUROC, accuracy, precision, recall, and F1 score (decision threshold 0.5) were again measured for each of these validation experiments, and metric distributions plotted using dot plots. Threshold recalibration for fairness was not performed because the output of the models were class probabilities rather than classifications.

A time cluster analysis was performed to assess out-of-sample performance, where the 15 models were trained on one of four time periods (2008-2010, 2011-2013, 2014-16, and 2017-2019), then validated on that period and the other three time periods, measuring AUROC, accuracy, precision, recall, and F1 score (decision threshold 0.5) - this was repeated with six random train-test splits for each pair of time periods, and metric distributions were plotted using dot plots.
\

## Appendix 4: Model specification for the discrete choice experiment ranked logit model

For the discrete choice experiment to produce component weights $w$, the ranking of each antibiotic (1-13) by participants was recorded and converted to a long format where each participant’s chosen rank order of antibiotics was treated as a series of choices using the ‘mlogit.data’ method from the ‘mlogit’ package.[@croissantEstimationRandomUtility2020] A multinomial ranked logit model was trained on the data to determine the relative importance of each of the six characteristics in influencing prescriber antibiotic rankings (again using ‘mlogit’). The model was trained to estimate maximum log likelihood using the Broyden–Fletcher–Goldfarb–Shanno (BFGS) method.[@daiPerfectExampleBFGS2013] 95% confidence intervals for characteristic importances were approximated using a bootstrap method with 1000 iterations. A subset analysis was performed where the approach was repeated on subsets of the data to build separate models for infection specialties, medicine, surgery, intensive care, and general practice to compare the weights of the six antimicrobial characteristics to different specialties.
\

## Appendix 5: Patient compositions of model development and microsimulation datasets

```{r, echo=FALSE,message=FALSE,warning=FALSE}

desc_tab <- read_csv("/Users/alexhoward/Documents/Projects/UDAST_code/uf_desctab.csv")

desc_tab$Characteristic[is.na(desc_tab$Characteristic)] <- "··"

knitr::kable(desc_tab)

```

### Appendix 5: Patient compositions of the model development and microsimulation datasets.
\

## Appendix 6: Antibiotic courses in the prescription dataset

```{r, echo=FALSE,message=FALSE,warning=FALSE}

ab_tab <- read_csv("/Users/alexhoward/Documents/Projects/UDAST_code/ab_tab.csv")

knitr::kable(ab_tab,)

```

### Appendix 6: Antibiotic courses in the prescription dataset, including two-combination administrations of more than 24 hours duration.
\

## Appendix 7: Final training hyperparameters

```{r, echo=FALSE,message=FALSE,warning=FALSE}

hyp_tab <- read_csv("/Users/alexhoward/Documents/Projects/UDAST_code/hyp_tab.csv")

knitr::kable(hyp_tab)

```

### Appendix 7: Final training hyperparameters used for the 39 clinical prediction models.
\

## Appendix 8: Feature importances

```{r, echo=FALSE,message=FALSE,warning=FALSE}

feat_table <- read_csv("/Users/alexhoward/Documents/Projects/UDAST_code/feat_table.csv")

knitr::kable(feat_table)

```

### Appendix 8: Feature importances in the clinical prediction models.
\

## Appendix 9: Small dataset stability metrics

```{r, echo=FALSE,message=FALSE,warning=FALSE}

knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/stability_AUC-ROC.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/stability_Precision.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/stability_Recall.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/stability_F1 score.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/stability_Accuracy.pdf")
  
```

### Appendix 9: Small dataset stability metrics for single antibiotic, CDI, and toxicity models in six random train-test splits stratified by outcome at a sequence of small dataset sizes across 50 training runs of XGBoost.
\

## Appendix 10: Year group cluster analysis

```{r, echo=FALSE,message=FALSE,warning=FALSE}

knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2008 - 2010_AUC-ROC.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2008 - 2010_Precision.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2008 - 2010_Recall.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2008 - 2010_F1 score.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2008 - 2010_Accuracy.pdf")

knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2011 - 2013_AUC-ROC.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2011 - 2013_Precision.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2011 - 2013_Recall.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2011 - 2013_F1 score.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2011 - 2013_Accuracy.pdf")

knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2014 - 2016_AUC-ROC.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2014 - 2016_Precision.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2014 - 2016_Recall.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2014 - 2016_F1 score.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2014 - 2016_Accuracy.pdf")

knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2017 - 2019_AUC-ROC.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2017 - 2019_Precision.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2017 - 2019_Recall.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2017 - 2019_F1 score.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2017 - 2019_Accuracy.pdf")

```

### Appendix 10: Year group cluster analysis of clinical prediction model performance in six random train-test splits stratified by outcome.
\

## Appendix 11: Fairness analyses

```{r, echo=FALSE,message=FALSE,warning=FALSE}

knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/protchar_Age group_AUC-ROC.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/protchar_Age group_Precision.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/protchar_Age group_Recall.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/protchar_Age group_F1 score.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/protchar_Age group_Accuracy.pdf")

knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/protchar_Race_AUC-ROC.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/protchar_Race_Precision.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/protchar_Race_Recall.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/protchar_Race_F1 score.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/protchar_Race_Accuracy.pdf")

knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/protchar_(Gender|Language|Marital)_AUC-ROC.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/protchar_(Gender|Language|Marital)_Precision.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/protchar_(Gender|Language|Marital)_Recall.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/protchar_(Gender|Language|Marital)_F1 score.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/protchar_(Gender|Language|Marital)_Accuracy.pdf")

  
```

### Appendix 11: Fairness analyses of clinical prediction model performance across age, race, gender, spoken language, and marital status in six random train-test splits stratified by outcome.
\

## Appendix 12: Coefficients from the ranked logit analysis

```{r, echo=FALSE,message=FALSE,warning=FALSE}

knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/Medicine_importances.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/Surgery_importances.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/Infection_importances.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/Intensive care_importances.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/General Practice_importances.pdf")

```

### Appendix 12: Coefficients from the ranked logit analysis of the discrete choice experiment across all prescriber specialties.
\

## Appendix 13: The distribution of utility values by specialty

```{r, echo=FALSE,message=FALSE,warning=FALSE}

knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/utility_Treatment_ (single agent, medical specialties).pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/utility_Treatment_ (single agent, surgical specialties).pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/utility_Treatment_ (single agent, infection specialties).pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/utility_Treatment_ (single agent, intensive care).pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/utility_Treatment_ (single agent, general practice).pdf")

```

### Appendix 13: The distribution of utility values across the microsimulation dataset for all single agents, stratified by the individual specialities.
\

## Appendix 14: Number of susceptible results in the top six recommendations

```{r, echo=FALSE,message=FALSE,warning=FALSE}

knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/UF_All agents_WHO access agents_plot.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/UF_IV agents_Oral agents_plot.pdf")

```

### Appendix 14: The number of susceptible results in the top six antimicrobial utility function (AUF) recommendations, overall / for WHO Access agents (top) and for oral / IV agents (bottom), compared to a standard panel of nitrofurantoin, trimethoprim-sulfamethoxazole, gentamicin, piperacillin-tazobactam, ceftriaxone, and ciprofloxacin. Median values are displayed as red/green dots, interquartile ranges as red/green lines, and all results as grey dot cloud darkness.
\

## References

::: {#refs}
:::
