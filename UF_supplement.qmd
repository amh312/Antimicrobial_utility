---
title: "Appendix"
format:
  docx:
    reference-doc: doc_template.docx
bibliography: UF_references.bib
csl: /Users/alexhoward/Zotero/styles/the-lancet-infectious-diseases.csl
editor: visual
---

```{r, echo=FALSE,message=FALSE,warning=FALSE}

library(tidyverse)
library(knitr)
library(glue)

```

## Appendix 1: Data processing workflow

```{r, echo=FALSE,message=FALSE,warning=FALSE}
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/Data flow chart.pdf")
```

### Appendix 1: Data processing workflow. Created in BioRender. Howard, A. (2025) https://BioRender.com/r77h648
\

## Appendix 2: Discrete choice experiment

This survey is designed to ascertain the relative importance of different features of an antimicrobial to clinicians when prescribing. It should take 2-5 minutes to complete.
Contextual information about the patient has intentionally been omitted - your decision should be made based on your experience of managing urinary tract infection in your usual clinical setting.
The factors listed are AWaRe classification*, risk of Clostridioides difficile diarrhoea, toxicity risk (e.g., nephrotoxicity), UTI-specificity (whether the drug should only be used for UTI), oral option, intravenous option, and financial cost.

* The WHO AWaRe classification assigns antibiotics ratings of Access, Watch and Reserve based on their risk of generating antimicrobial resistance. Those in the Access category are the lowest risk, those in Reserve are the highest.

1. A patient has urinary tract infection. You have no other details about the patient at this stage (including disease severity). Please rank the following fictional antibiotic treatments from 1 to 13 in order of most to least preferred in this scenario by dragging and dropping options or using the arrows on the right. When finished, just click the 'Done' button at the bottom of the page.

```{r, echo=FALSE,message=FALSE,warning=FALSE}

knitr::include_graphics(c(
  "/Users/alexhoward/Documents/Projects/UDAST_code/S1.png",
  "/Users/alexhoward/Documents/Projects/UDAST_code/S2.png",
  "/Users/alexhoward/Documents/Projects/UDAST_code/S3.png",
  "/Users/alexhoward/Documents/Projects/UDAST_code/S4.png",
  "/Users/alexhoward/Documents/Projects/UDAST_code/S5.png",
  "/Users/alexhoward/Documents/Projects/UDAST_code/S6.png",
  "/Users/alexhoward/Documents/Projects/UDAST_code/S7.png",
  "/Users/alexhoward/Documents/Projects/UDAST_code/S8.png",
  "/Users/alexhoward/Documents/Projects/UDAST_code/S9.png",
  "/Users/alexhoward/Documents/Projects/UDAST_code/S10.png",
  "/Users/alexhoward/Documents/Projects/UDAST_code/S11.png",
  "/Users/alexhoward/Documents/Projects/UDAST_code/S12.png",
  "/Users/alexhoward/Documents/Projects/UDAST_code/S13.png"
                          ))
                      

```

\

## Appendix 3: Model specification and training/testing/validation details for clinical prediction models

A total of 15 individual models were required for the primary analysis. 13 of these models were individual antibiotic susceptibility prediction models that predicted probability of antibiotic activity against the urinary pathogen (i.e., pathogen susceptibility to the antibiotic), and were trained on the MIMIC-IV urine culture dataset. A *Clostridioides difficile* infection (CDI) prediction model and an antibiotic toxicity prediction model were trained on the MIMIC-IV prescriptions dataset.

The outcome for all antibiotic susceptibility prediction models was the probability of an 'S' result indicating susceptibility of the organism grown in that urine specimen to that antibiotic. The outcome for the CDI prediction model was the probability of a positive *C. difficile* stool result within three months following the start date/time of an antibiotic. The outcome for the antibiotic toxicity prediction model was the probability of a composite outcome of either stage three acute kidney injury (a new increase in serum creatinine to at least three times baseline or at least 3.54µmol/dL in the absence of co-administration of nephrotoxic drugs as defined by the British National Formulary or intravenous contrast during the associated hospital admission), deranged liver function tests (a result newly above the upper end of the normal range for alanine aminotransferase, aspartate aminotransferase, or alkaline phosphatase in the absence of previous coded chronic liver disease or biliary instrumentation in the associated hospital admission), marrow suppression (new anaemia, leukopenia, or thrombocytopenia in the absence of co-administration of cytotoxic drugs as defined by the British National Formulary or coded bleeding diagnosis in the associated hospital admission) in the seven days following the start date of an antibiotic, or a coded antibiotic adverse event for the associated hospital admission.[@Appendix1Interactions,@kongIncidenceCharacteristicsRisk2021, @tammaAssociationAdverseEvents2017]

Choices of predictor variables and their time horizons were based on indirect or direct causal plausibility and/or association with outcome variables — this process was undertaken by the lead author (Consultant in Medical Microbiology, male, 30s, white British) and reviewed by co-authors (all male in the age range 20-60 with a racial mix of white British, White Australian, Maltese, Indian, and Chinese).[@howardPersonalisedAntimicrobialSusceptibility2024] Outcome and predictor variables were selected consistently across sociodemographic groups. No blinding to allocation or predictor/outcome assessment was implemented at any stage. Allocation to training and validation datasets was performed individually for each model by random 80:20 split without replacement, stratified to maintain similar proportions of the outcome in the training and validation datasets.

We have described clinical prediction models of resistance for 12 of the 13 antimicrobial agents using logistic regression in our previous work.[@howardPersonalisedAntimicrobialSusceptibility2024] In an attempt to capture more non-linear relationships in the data, XGBoost (an ensemble method that improves predictive accuracy by sequentially fitting decision trees in so-called boosting rounds and combining their predictions, implemented via the ‘xgboost’ package <https://cran.r-project.org/web/packages/xgboost/index.html>), was used for this study.[@chenXgboostExtremeGradient2014, @chenXGBoostScalableTree2016] XGBoost models underwent hyperparameter tuning and training on the training datasets using L2 regularisation (ridge penalty) to control overfitting. Area under the receiver operating characteristic curve (AUROC) was used as the model evaluation metric. Class imbalance methods (e.g., class weighting) were not used.

Hyperparameter tuning was performed sequentially using AUROC for each of the 15 models across hyperparameters in three stages to reduce the number of hyperparameter combinations, and therefore computational time:

1.  Ten maximum tree depth and minimum child weight values in the range two to nine and one to 10 respectively were selected using Latin hypercube sampling (LHS) with 'randomLHS' from the 'lhs' package (<https://cran.r-project.org/web/packages/lhs/index.html>) and tested using 5-fold cross validation across 50 boosting rounds (learning rate 0.05, subsample row ratio 0.8, subsample columns ratio 0.8).
2.  Ten subsample row and column ratio values in the range 0.5-1 were selected and tested using the same method as above.
3.  To balance computational time with performance, learning rates were tested by starting at a value of 0.1 then halving them or adding 0.1 as required (up to a maximum of 0.3) until the number of boosting rounds at which AUROC did not improve for 50 rounds was between 200 and 1,000. If models did not converge within 1,000 boosting rounds at a learning rate of 0.3, the process was started again at 0.1 but with a fixed maximum tree depth of 6 to limit model complexity and speed up convergence.

Predictor variable contributions to predictive value (feature importances) in model training were measured using Shapley values with 'predict' from the 'stats' package.[@Chapter53Shapley] Predictor variables with Shapley values of zero were then excluded from models for the final boosting round.

A single validation run was performed with the final trained model on the validation dataset, measuring AUROC, accuracy, precision, recall, and F1 score (decision threshold 0.5), with 95% confidence intervals approximated using bootstrapping.

A model stability analysis was performed to assess the performance of each model when trained on a smaller training dataset. Training and validation of the final model was performed in six random train-test splits without replacement (stratified by outcome) for each of four smaller train-test dataset size ratios (2:98%, 6:94%, 10:90%, and 14:86%) — AUROC, accuracy, precision, recall, and F1 score (decision threshold 0.5) were measured for each of these validations, and metric distributions were plotted using dot plots to assess heterogeneity in performance.

A model fairness analysis was performed where each of the same 15 trained models was validated separately across a range of protected characteristics (race, age, marital status, first language, and gender), with six random-train-test splits per characteristic — AUROC, accuracy, precision, recall, and F1 score (decision threshold 0.5) were again measured for each of these validation experiments, and metric distributions plotted using dot plots. Threshold recalibration for fairness was not performed because the output of the models were class probabilities rather than classifications.

A time cluster analysis was performed to assess out-of-sample performance, where the 15 models were trained on one of four time periods (2008-2010, 2011-2013, 2014-16, and 2017-2019), then validated on holdout datasets from that period and the other three time periods, measuring AUROC, accuracy, precision, recall, and F1 score (decision threshold 0.5) — this was repeated with six random train-test splits for each pair of time periods, and metric distributions were plotted using dot plots.
\

## Appendix 4: Model specification for the discrete choice experiment ranked logit model

For the discrete choice experiment to produce component weights $w$, the ranking of each antibiotic (1-13) by participants was recorded and converted to a long format where each participant’s chosen rank order of antibiotics was treated as a series of choices using the ‘mlogit.data’ method from the ‘mlogit’ package.[@croissantEstimationRandomUtility2020] A multinomial ranked logit model was trained on the data to determine the relative importance of each of the six characteristics in influencing participant antibiotic rankings (again using ‘mlogit’). The model was trained to estimate maximum log likelihood using the Broyden–Fletcher–Goldfarb–Shanno (BFGS) method.[@daiPerfectExampleBFGS2013] 95% confidence intervals for characteristic importances were approximated using a bootstrap method with 1000 iterations. A subset analysis was performed where the approach was repeated on subsets of the data to build separate models for infection specialties, medicine, surgery, intensive care, and general practice to compare the importance of the six antimicrobial characteristics to different specialties.
\

## Appendix 5: Effect in changes of probabilities and weights on antimicrobial utility
```{r, echo=FALSE,message=FALSE,warning=FALSE}

knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/wk_plot.pdf")
  
```
### Appendix 5: The effect of variation in *w* on the value of *u(w,k)* for three different values of *k* represented by the solid, dotted, and dashed lines.
\

## Appendix 6: Patient compositions of model development and microsimulation datasets

```{r, echo=FALSE,message=FALSE,warning=FALSE}

desc_tab <- read_csv("/Users/alexhoward/Documents/Projects/UDAST_code/uf_desctab.csv")

desc_tab$Characteristic[is.na(desc_tab$Characteristic)] <- "··"

knitr::kable(desc_tab)

```

### Appendix 6: Patient compositions of the model development and microsimulation datasets.
\

## Appendix 7: Antibiotic courses in the prescription dataset

```{r, echo=FALSE,message=FALSE,warning=FALSE}

ab_tab <- read_csv("/Users/alexhoward/Documents/Projects/UDAST_code/ab_tab.csv")

knitr::kable(ab_tab,)

```

### Appendix 7: Antibiotic courses in the prescription dataset, including two-combination administrations of more than 24 hours duration.
\

## Appendix 8: Final training hyperparameters

```{r, echo=FALSE,message=FALSE,warning=FALSE}

hyp_tab <- read_csv("/Users/alexhoward/Documents/Projects/UDAST_code/hyp_tab_singles.csv")

knitr::kable(hyp_tab)

```

### Appendix 8: Final training hyperparameters used for the clinical prediction models.
\

## Appendix 9: Receiver operating characteristic curves for the 15 clinical prediction models used in the utility function
```{r, echo=FALSE,message=FALSE,warning=FALSE}

knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/roc_plots_main.pdf")
  
```

### Appendix 9: Receiver operating characteristic curves (black lines) for probability predictions made for antibiotic susceptibility, *C. difficile* infection (CDI), and toxicity on the validation dataset, with chance level (performance level if the model had no predictive value) represented by grey dashed lines.


##Appendix 10: Calibration curves for the 15 clinical prediction models used in the utility function
```{r, echo=FALSE,message=FALSE,warning=FALSE}

knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/calibration_plots_main.pdf")
  
```
### Appendix 10: Calibration curves (green lines) for probability predictions made for antibiotic susceptibility, *C. difficile* infection (CDI), and toxicity on the validation dataset, with red dots representing means of probability groups, ideal calibration represented by grey dashed lines, and slope of a fitted linear model in inset boxes.

## Appendix 11: Feature importances

```{r, echo=FALSE,message=FALSE,warning=FALSE}

feat_table <- read_csv("/Users/alexhoward/Documents/Projects/UDAST_code/feat_table_singles.csv")

knitr::kable(feat_table)

```

### Appendix 11: Feature importances in the clinical prediction models.
\

## Appendix 12: Small dataset stability metrics

```{r, echo=FALSE,message=FALSE,warning=FALSE}

knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/stability_AUC-ROC.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/stability_Precision.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/stability_Recall.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/stability_F1 score.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/stability_Accuracy.pdf")
  
```

### Appendix 12: Small dataset stability metrics for single antibiotic, CDI, and toxicity models in six random train-test splits stratified by outcome at a sequence of small dataset sizes across 50 training runs of XGBoost.
\

## Appendix 13: Year group cluster analysis

```{r, echo=FALSE,message=FALSE,warning=FALSE}

knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2008 - 2010_AUC-ROC.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2008 - 2010_Precision.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2008 - 2010_Recall.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2008 - 2010_F1 score.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2008 - 2010_Accuracy.pdf")

knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2011 - 2013_AUC-ROC.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2011 - 2013_Precision.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2011 - 2013_Recall.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2011 - 2013_F1 score.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2011 - 2013_Accuracy.pdf")

knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2014 - 2016_AUC-ROC.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2014 - 2016_Precision.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2014 - 2016_Recall.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2014 - 2016_F1 score.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2014 - 2016_Accuracy.pdf")

knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2017 - 2019_AUC-ROC.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2017 - 2019_Precision.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2017 - 2019_Recall.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2017 - 2019_F1 score.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/timesens_2017 - 2019_Accuracy.pdf")

```

### Appendix 13: Year group cluster analysis of clinical prediction model performance in six random train-test splits stratified by outcome.
\

## Appendix 14: Fairness analyses

```{r, echo=FALSE,message=FALSE,warning=FALSE}

knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/protchar_Age group_AUC-ROC.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/protchar_Age group_Precision.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/protchar_Age group_Recall.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/protchar_Age group_F1 score.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/protchar_Age group_Accuracy.pdf")

knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/protchar_Race_AUC-ROC.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/protchar_Race_Precision.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/protchar_Race_Recall.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/protchar_Race_F1 score.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/protchar_Race_Accuracy.pdf")

knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/protchar_(Gender|Language|Marital)_AUC-ROC.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/protchar_(Gender|Language|Marital)_Precision.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/protchar_(Gender|Language|Marital)_Recall.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/protchar_(Gender|Language|Marital)_F1 score.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/protchar_(Gender|Language|Marital)_Accuracy.pdf")

  
```

### Appendix 14: Fairness analyses of clinical prediction model performance across age, race, gender, spoken language, and marital status in six random train-test splits stratified by outcome.
\

## Appendix 15: Coefficients from the ranked logit analysis

```{r, echo=FALSE,message=FALSE,warning=FALSE}

knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/Medicine_importances.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/Surgery_importances.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/Infection_importances.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/Intensive care_importances.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/General Practice_importances.pdf")

```

### Appendix 15: Coefficients from the ranked logit analysis of the discrete choice experiment across all prescriber specialties.
\

## Appendix 16: The distribution of utility values by specialty

```{r, echo=FALSE,message=FALSE,warning=FALSE}

knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/utility_Treatment_ (single agent, medical specialties).pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/utility_Treatment_ (single agent, surgical specialties).pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/utility_Treatment_ (single agent, infection specialties).pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/utility_Treatment_ (single agent, intensive care).pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/utility_Treatment_ (single agent, general practice).pdf")

```

### Appendix 16: The distribution of utility values across the microsimulation dataset for all single agents, stratified by the individual specialities.
\

## Appendix 17: Number of susceptible results in the top six recommendations

```{r, echo=FALSE,message=FALSE,warning=FALSE}

knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/UF_All agents_WHO access agents_plot.pdf")
knitr::include_graphics("/Users/alexhoward/Documents/Projects/UDAST_code/UF_IV agents_Oral agents_plot.pdf")

```

### Appendix 17: The number of susceptible results in the top six antimicrobial utility function (AUF) recommendations, overall / for WHO Access agents (top) and for oral / IV agents (bottom), compared to a standard panel of nitrofurantoin, trimethoprim-sulfamethoxazole, gentamicin, piperacillin-tazobactam, ceftriaxone, and ciprofloxacin. Median values are displayed as red/green dots, interquartile ranges as red/green lines, and all results as grey dot cloud darkness.
\

## References

::: {#refs}
:::
